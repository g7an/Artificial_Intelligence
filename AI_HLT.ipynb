{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HLT_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g7an/artificial_intelligence/blob/main/AI_HLT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpUkaBBUanNv"
      },
      "source": [
        "<center> <h1> Language Models </h1> </center>\n",
        "\n",
        "In this homework, we will train character-level language model\n",
        "on a lyrics dataset.\n",
        "\n",
        "More specifically, we will:\n",
        "1. Implement a character-level language model based on LSTM.\n",
        "2. Train it on a lyrics dataset.\n",
        "3. Sample previously unseen lyrics from our model.\n",
        "4. Augment our model with an artist information.\n",
        "\n",
        "#### Google colaboratory\n",
        "\n",
        "Before getting started, get familiar with google colaboratory:\n",
        "https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "This is a neat python environment that works in the cloud and does not require you to\n",
        "set up anything on your personal machine\n",
        "(it also has some built-in IDE features that make writing code easier).\n",
        "Moreover, it allows you to copy any existing collaboratory file, alter it and share\n",
        "with other people. In this homework, we will ask you to copy current colaboraty,\n",
        "complete all the tasks and share your colaboratory notebook with us so\n",
        "that we can grade it. We will also use colaboratory for the homework #4 so this\n",
        "homework will prepare you for it.\n",
        "\n",
        "#### Submission\n",
        "\n",
        "Before you start working on this homework do the following steps:\n",
        "\n",
        "1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n",
        "2. Follow all the steps in this collaboratory file and write/change/uncomment code as necessary.\n",
        "3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n",
        "4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected.\n",
        "5. Paste the link into your submission pdf file so that we can view it and grade.\n",
        "\n",
        "<center> <h2> Problem statement </h2> </center>\n",
        "\n",
        "In this homework we will train character-level language RNN model on lyrics dataset of most some artists. Having a trained model, we will sample a couple of songs which will be a mixture of different styles of different artists. After that we will update our model to become a conditional character-level RNN, making it possible for us to sample songs conditioned on artist.\n",
        "\n",
        "<center> <h2> Lyrics dataset </h2> </center>\n",
        "\n",
        "For our experiments we will use a subset of [Song Lyrics Kaggle dataset](https://www.kaggle.com/mousehead/songlyrics) which contains good variety of recent artists and more older ones. It is stored as a pandas file and we wrote a python wrapper around it to be able to use it for training purposes.\n",
        "\n",
        "<center> <h2> Character-Level language model </h2> </center>\n",
        "\n",
        "![alt text](http://warmspringwinds.github.io/assets/img/character_level_model.jpg \"Logo Title Text 1\")\n",
        "\n",
        "Before choosing a model, let’s have a closer look at our task. Given current letter and all previous letters, we will try to predict the next character. During training we will just take a sequence, and use all its characters except the last one as an input and the same sequence starting from the second character as groundtruth (see the picture above; Source).\n",
        "\n",
        "Our language model is defined on a character level. We will create a dictionary which will contain all English characters plus some special symbols, like period, comma, and end-of-line symbol. Each charecter will be represented as one-hot-encoded tensor. For more information about character-level models and examples, refer to the [the following resource](https://github.com/spro/practical-pytorch)\n",
        "\n",
        "\n",
        "Having characters, we can now form sequences of characters. What we would actually like to model is $p(current letter|all previous letters)$. At first, the task seems intractable as the number of previous letters is variable and it might become really large in case of long sequences. Turns out Reccurent Neural Netoworks can tackle this problem to a certain extent by using shared weights and fixed size hidden state. This leads us to a next section dedicated to RNNs.\n",
        "\n",
        "<center> <h2> Recurrent Neural Networks </h2> </center>\n",
        "\n",
        "![alt text](http://warmspringwinds.github.io/assets/img/rnn_unfold.jpg \"Logo Title Text 1\")\n",
        "\n",
        "Recurrent neural networks are a family of neural networks for processing sequential data. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Because of arbitrary size input sequences, they are concisely depicted as a graph with a cycle (see the picture; Source). But they can be “unfolded” if the size of input sequence is known. They define a non-linear mapping from a current input xt and previous hidden state st−1 to the output ot and current hidden state st. Hidden state size has a predefined size and stores features which are updated on each step and affect the result of mapping.\n",
        "\n",
        "Now align the previous picture of the character-level language model and the unfolded RNN picture to see how we are using the RNN model to learn a character level language model.\n",
        "\n",
        "While the picture depicts the Vanilla RNN, we will use LSTM in our work as it is easier to train usually achieves better results.\n",
        "\n",
        "For a more elaborate introduction to RNNs, we refer reader to [the following resource](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/).\n",
        "\n",
        "<center> <h2> Training unconditional character-level language model </h2> </center>\n",
        "\n",
        "Our first experiment consisted of training of our character-level language model RNN on the whole corpus. We didn’t take into consideration the artist information while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn83BquKZYC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b4aa6c-a913-494a-81ae-15aa7dbc18f3"
      },
      "source": [
        "# Downloading dataset and installing dependencies\n",
        "!wget https://www.dropbox.com/s/ge1bhvik5jya9hr/songdata.csv?dl=0\n",
        "!mv songdata.csv\\?dl\\=0 songdata.csv\n",
        "!pip install livelossplot==0.3.4\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-27 15:46:10--  https://www.dropbox.com/s/ge1bhvik5jya9hr/songdata.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ge1bhvik5jya9hr/songdata.csv [following]\n",
            "--2021-11-27 15:46:10--  https://www.dropbox.com/s/raw/ge1bhvik5jya9hr/songdata.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc850feb0666c6edd04d5ab226c5.dl.dropboxusercontent.com/cd/0/inline/BaxFPu-uK4SyBChsU8vHfSmJSRYDR2BEU9TjNDgy5UAmxCyK1eNApq5xcsSF-9mRYQ5iNc9jImpRcMV3_7TdNjln1qA3F6IW9f7ecOS8X4Q2dZs8epcbf-4PjgFNQaysR0I045bvHJhLN_Gp65GaaRVY/file# [following]\n",
            "--2021-11-27 15:46:10--  https://uc850feb0666c6edd04d5ab226c5.dl.dropboxusercontent.com/cd/0/inline/BaxFPu-uK4SyBChsU8vHfSmJSRYDR2BEU9TjNDgy5UAmxCyK1eNApq5xcsSF-9mRYQ5iNc9jImpRcMV3_7TdNjln1qA3F6IW9f7ecOS8X4Q2dZs8epcbf-4PjgFNQaysR0I045bvHJhLN_Gp65GaaRVY/file\n",
            "Resolving uc850feb0666c6edd04d5ab226c5.dl.dropboxusercontent.com (uc850feb0666c6edd04d5ab226c5.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc850feb0666c6edd04d5ab226c5.dl.dropboxusercontent.com (uc850feb0666c6edd04d5ab226c5.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/plain]\n",
            "Saving to: ‘songdata.csv?dl=0’\n",
            "\n",
            "songdata.csv?dl=0   100%[===================>]  69.08M  34.8MB/s    in 2.0s    \n",
            "\n",
            "2021-11-27 15:46:13 (34.8 MB/s) - ‘songdata.csv?dl=0’ saved [72436445/72436445]\n",
            "\n",
            "Collecting livelossplot==0.3.4\n",
            "  Downloading livelossplot-0.3.4-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.3.4) (5.3.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.3.4) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.3.4) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->livelossplot==0.3.4) (1.15.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (4.10.1)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.3.5)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.1.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.1.1)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (4.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (2.11.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (5.1.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->livelossplot==0.3.4) (0.12.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook->livelossplot==0.3.4) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->livelossplot==0.3.4) (0.7.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->notebook->livelossplot==0.3.4) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot==0.3.4) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->livelossplot==0.3.4) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook->livelossplot==0.3.4) (4.1.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->livelossplot==0.3.4) (2.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.3.4) (21.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.3.4) (0.5.1)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.3.4\n",
            "Sat Nov 27 15:46:18 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECL2lbFPUF42"
      },
      "source": [
        "First, let us create a dictionary, we will use $100$ characters and some special symbols including $\\n$ which will allow our generator to also decide when the current line should end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DRgqtnoUmzT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70d8477-5c0d-49da-de05-9d3f3b4ea2b8"
      },
      "source": [
        "import torch\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "all_characters = string.printable\n",
        "number_of_characters = len(all_characters)\n",
        "\n",
        "print(all_characters)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
            "\r\u000b\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4VmJMSmVGMa"
      },
      "source": [
        "Below we will define some helper functions that will help us to convert character to corresponding\n",
        "labels that we will use for actual training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl5JpbLFVZ45"
      },
      "source": [
        "def character_to_label(character):\n",
        "    \n",
        "    character_label = all_characters.find(character)\n",
        "        \n",
        "    return character_label\n",
        "\n",
        "\n",
        "def string_to_labels(character_string):\n",
        "    \n",
        "    return list(map(lambda character: character_to_label(character), character_string))\n",
        "\n",
        "  \n",
        "def pad_sequence(seq, max_length, pad_label=100):\n",
        "    \n",
        "    seq += [pad_label for i in range(max_length - len(seq))]\n",
        "    \n",
        "    return seq"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QcAVw7AVhK8"
      },
      "source": [
        "Now we will define a dataset class that will take care of loading data from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PioOgKILVqfv"
      },
      "source": [
        "\n",
        "# The class works in two modes: train and validation, which we will\n",
        "# use during training\n",
        "\n",
        "# It also padds data so that all sequences are of the same length.\n",
        "# We need this because otherwise efficient batching will not work\n",
        "# and we will not be able to use GPU processing to the full extent\n",
        "class LyricsGenerationDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file_path,\n",
        "                 minimum_song_count=None,\n",
        "                 artists=None,\n",
        "                 train=True):\n",
        "        \n",
        "        \n",
        "        self.lyrics_dataframe = pd.read_csv(csv_file_path)\n",
        "        \n",
        "        if artists:\n",
        "            \n",
        "            self.lyrics_dataframe = self.lyrics_dataframe[self.lyrics_dataframe.artist.isin(artists)]\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        if minimum_song_count:\n",
        "        \n",
        "            # Getting artists that have 70+ songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.groupby('artist').filter(lambda x: len(x) > minimum_song_count)\n",
        "            # Reindex .loc after we fetched random songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        # Get the length of the biggest lyric text\n",
        "        # We will need that for padding\n",
        "        self.max_text_len = self.lyrics_dataframe.text.str.len().max()\n",
        "        \n",
        "        whole_dataset_len = len(self.lyrics_dataframe)\n",
        "        \n",
        "        self.indexes = range(whole_dataset_len)\n",
        "        \n",
        "        if train:\n",
        "          \n",
        "          self.indexes = self.indexes[500:]\n",
        "          \n",
        "        else:\n",
        "          self.indexes = self.indexes[:500]\n",
        "        \n",
        "        self.artists_list = list(self.lyrics_dataframe.artist.unique())\n",
        "        \n",
        "        self.number_of_artists = len(self.artists_list)\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.indexes)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        index = self.indexes[index]\n",
        "        \n",
        "        sequence_raw_string = self.lyrics_dataframe.loc[index].text\n",
        "        \n",
        "        sequence_string_labels = string_to_labels(sequence_raw_string)\n",
        "        \n",
        "        sequence_length = len(sequence_string_labels) - 1\n",
        "        \n",
        "        # Shifted by one char\n",
        "        input_string_labels = sequence_string_labels[:-1]\n",
        "        output_string_labels = sequence_string_labels[1:]\n",
        "                \n",
        "        # pad sequence so that all of them have the same lenght\n",
        "        # Otherwise the batching won't work\n",
        "        input_string_labels_padded = pad_sequence(input_string_labels, max_length=self.max_text_len)\n",
        "        \n",
        "        # Filling in sequences with a '-100' lable -- this way it is omitted\n",
        "        # in cross_entropy_loss\n",
        "        output_string_labels_padded = pad_sequence(output_string_labels, max_length=self.max_text_len, pad_label=-100)\n",
        "        \n",
        "        return (torch.LongTensor(input_string_labels_padded),\n",
        "                torch.LongTensor(output_string_labels_padded),\n",
        "                torch.LongTensor([sequence_length]) )\n",
        "\n",
        "# Here are the artists\n",
        "artists = [\n",
        "'ABBA',\n",
        "'Ace Of Base',\n",
        "'Backstreet Boys',\n",
        "'Bob Marley',\n",
        "'Bon Jovi',\n",
        "'Britney Spears',\n",
        "'Bruno Mars',\n",
        "'Coldplay',\n",
        "'Ed Sheeran',\n",
        "'Elton John',\n",
        "'Elvis Presley',\n",
        "'Eminem',\n",
        "'Evanescence',\n",
        "'Fall Out Boy',\n",
        "'Foo Fighters',\n",
        "'Green Day',\n",
        "'HIM',\n",
        "'Imagine Dragons',\n",
        "'Justin Bieber',\n",
        "'Justin Timberlake',\n",
        "'Katy Perry',\n",
        "'Lady Gaga',\n",
        "'Lana Del Rey',\n",
        "'Linkin Park',\n",
        "'Madonna',\n",
        "'Marilyn Manson',\n",
        "'Maroon 5',\n",
        "'Metallica',\n",
        "'Michael Jackson',\n",
        "'Nickelback',\n",
        "'Oasis',\n",
        "'One Direction',\n",
        "'P!nk',\n",
        "'Queen',\n",
        "'Red Hot Chili Peppers',\n",
        "'Rihanna',\n",
        "'Robbie Williams',\n",
        "'Sting',\n",
        "'The Script',\n",
        "'Weezer',\n",
        "'Yellowcard']\n",
        "\n",
        "trainset = LyricsGenerationDataset(csv_file_path='songdata.csv', artists=artists)\n",
        "valset = LyricsGenerationDataset(csv_file_path='songdata.csv', artists=artists, train=False)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcRWxDZmWZN8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "outputId": "96f24a34-5880-4cc1-c2b7-afd302d8f466"
      },
      "source": [
        "# Let us inspect the dataset quickly\n",
        "trainset.lyrics_dataframe"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>Ahe's My Kind Of Girl</td>\n",
              "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
              "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>Andante, Andante</td>\n",
              "      <td>/a/abba/andante+andante_20002708.html</td>\n",
              "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>As Good As New</td>\n",
              "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
              "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang</td>\n",
              "      <td>/a/abba/bang_20598415.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>ABBA</td>\n",
              "      <td>Bang-A-Boomerang</td>\n",
              "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
              "      <td>Making somebody happy is a question of give an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4793</th>\n",
              "      <td>57175</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>The Finish Line</td>\n",
              "      <td>/y/yellowcard/the+finish+line_10195563.html</td>\n",
              "      <td>Hello friend, it's been too long and every tow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4794</th>\n",
              "      <td>57176</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>Twenty Three</td>\n",
              "      <td>/y/yellowcard/twenty+three_10195543.html</td>\n",
              "      <td>I got to tell you that he waited all his life ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>57177</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>Waiting Game</td>\n",
              "      <td>/y/yellowcard/waiting+game_20423993.html</td>\n",
              "      <td>You and me  \\nA little different  \\nThough we ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>57178</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>Way Away</td>\n",
              "      <td>/y/yellowcard/way+away_10195536.html</td>\n",
              "      <td>I think I'm breaking out  \\nI'm gonna leave yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>57179</td>\n",
              "      <td>Yellowcard</td>\n",
              "      <td>Words, Hands, Hearts</td>\n",
              "      <td>/y/yellowcard/words+hands+hearts_20424033.html</td>\n",
              "      <td>The whole world was sleeping  \\nAnd I was ther...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4798 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      index  ...                                               text\n",
              "0         0  ...  Look at her face, it's a wonderful face  \\nAnd...\n",
              "1         1  ...  Take it easy with me, please  \\nTouch me gentl...\n",
              "2         2  ...  I'll never know why I had to go  \\nWhy I had t...\n",
              "3         3  ...  Making somebody happy is a question of give an...\n",
              "4         4  ...  Making somebody happy is a question of give an...\n",
              "...     ...  ...                                                ...\n",
              "4793  57175  ...  Hello friend, it's been too long and every tow...\n",
              "4794  57176  ...  I got to tell you that he waited all his life ...\n",
              "4795  57177  ...  You and me  \\nA little different  \\nThough we ...\n",
              "4796  57178  ...  I think I'm breaking out  \\nI'm gonna leave yo...\n",
              "4797  57179  ...  The whole world was sleeping  \\nAnd I was ther...\n",
              "\n",
              "[4798 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVfxEdLIC_TR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37b67bf-e2fc-4040-da23-e468e925452b"
      },
      "source": [
        "len(trainset)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4298"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-O9_bPEEeak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38be153-1ace-41d2-adf2-fbaf7602bd97"
      },
      "source": [
        "len(valset)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTikd1ZRa6tC"
      },
      "source": [
        "Below, we are defining an RNN class, and will ask you to feel out missing parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4ReYbD3a5ka"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 input_size=101,\n",
        "                 hidden_size=512,\n",
        "                 num_classes=100,\n",
        "                 n_layers=2):\n",
        "        \n",
        "        # input_size = 101 -- 100 characters + background character\n",
        "        # num_classes = 100 -- we predict what character goes next\n",
        "        \n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        \n",
        "        # input_size -- size of the dictionary + 1 (accounts for padding constant)\n",
        "        \n",
        "        # Below use nn.Embedding to map from input_size to hidden_size\n",
        "        # nn.Embedding converts labels into one-hot encoding and runs a linear\n",
        "        # layer on each of the converted one-hot encoded elements\n",
        "        self.embedder = nn.Embedding(self.input_size, self.hidden_size, 100) # TODO\n",
        "        \n",
        "        # Below use nn.LSTM that accepts hidden_size as input size,\n",
        "        # and has hidden size equal to hidden_size argument and\n",
        "        # n_layers\n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.n_layers) # TODO\n",
        "        \n",
        "        # Below use nn.Linear to make representation of hidden_size\n",
        "        # to the number of classes that will be fed to softmax\n",
        "        # to decide which character goes next\n",
        "        self.logits_fc = nn.Linear(self.hidden_size, self.num_classes) # TODO\n",
        "    \n",
        "    \n",
        "    def forward(self,\n",
        "                input_sequences,\n",
        "                input_sequences_lengths,\n",
        "                hidden=None):\n",
        "        \n",
        "        batch_size = input_sequences.shape[1]\n",
        "\n",
        "        embedded = self.embedder(input_sequences)\n",
        "        \n",
        "        # This is needed for efficient processing of sequences of\n",
        "        # variable lengths. Feel free to skip\n",
        "        # Here we run rnns only on non-padded regions of the batch\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_sequences_lengths)\n",
        "        outputs, hidden = self.lstm(packed, hidden)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
        "        \n",
        "        \n",
        "        logits = self.logits_fc(outputs)\n",
        "        # This is needed for cross entropy loss\n",
        "        logits = logits.transpose(0, 1).contiguous()\n",
        "        logits_flatten = logits.view(-1, self.num_classes)\n",
        "        \n",
        "        return logits_flatten, hidden\n",
        "\n",
        "# for efficient processing of sequences of\n",
        "# variable lengths. Feel free to skip \n",
        "def post_process_sequence_batch(batch_tuple):\n",
        "  \n",
        "  input_sequences, output_sequences, lengths = batch_tuple\n",
        "\n",
        "  splitted_input_sequence_batch = input_sequences.split(split_size=1)\n",
        "  splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
        "  splitted_lengths_batch = lengths.split(split_size=1)\n",
        "\n",
        "  training_data_tuples = zip(splitted_input_sequence_batch,\n",
        "                             splitted_output_sequence_batch,\n",
        "                             splitted_lengths_batch)\n",
        "\n",
        "  training_data_tuples_sorted = sorted(training_data_tuples,\n",
        "                                       key=lambda p: int(p[2]),\n",
        "                                       reverse=True)\n",
        "\n",
        "  splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
        "\n",
        "  input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
        "  output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
        "  lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
        "\n",
        "  input_sequence_batch_sorted = input_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "  output_sequence_batch_sorted = output_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "\n",
        "  input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
        "\n",
        "  # pytorch's api for rnns wants lenghts to be list of ints\n",
        "  lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
        "  lengths_batch_sorted_list = list(map(lambda x: int(x), lengths_batch_sorted_list))\n",
        "\n",
        "\n",
        "  return input_sequence_batch_transposed, output_sequence_batch_sorted, lengths_batch_sorted_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYFqPtAGkZuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04dc2af8-8324-4890-802b-a4bf9edab309"
      },
      "source": [
        "trainset_loader = torch.utils.data.DataLoader(trainset,\n",
        "                                              batch_size=50,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=4,\n",
        "                                              drop_last=True)\n",
        "\n",
        "\n",
        "valset_loader = torch.utils.data.DataLoader(valset,\n",
        "                                            batch_size=50,\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=1,\n",
        "                                            drop_last=True)\n",
        "\n",
        "\n",
        "rnn = RNN(input_size=len(all_characters) + 1, hidden_size=512, num_classes=len(all_characters))\n",
        "rnn.cuda()\n",
        "\n",
        "#TODO? Try different learning rates and optimizers\n",
        "learning_rate = 0.003\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss().cuda()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcZ4YAw6qqSZ"
      },
      "source": [
        "from livelossplot import PlotLosses\n",
        "\n",
        "liveloss = PlotLosses()\n",
        "\n",
        "# We will get back to this function later on,\n",
        "# Just ignore it so far -- we will use it to generate\n",
        "# samples during our training\n",
        "def sample_from_rnn(starting_string=\"I\", sample_length=300, temperature=0.5):\n",
        "    assert temperature >= 0.0\n",
        "    sampled_string = starting_string\n",
        "    hidden = None\n",
        "\n",
        "    first_input = torch.LongTensor( string_to_labels(starting_string) ).cuda()\n",
        "    first_input = first_input.unsqueeze(1)\n",
        "    current_input = first_input\n",
        "\n",
        "    output, hidden = rnn(current_input, [len(sampled_string)], hidden=hidden)\n",
        "\n",
        "    output = output[-1, :].unsqueeze(0)\n",
        "\n",
        "    for i in range(sample_length):\n",
        "\n",
        "        output_dist = nn.functional.softmax( output.view(-1).div(temperature + 1e-4) ).data\n",
        "\n",
        "        predicted_label = torch.multinomial(output_dist, 1)\n",
        "\n",
        "        sampled_string += all_characters[int(predicted_label[0])]\n",
        "\n",
        "        current_input = predicted_label.unsqueeze(1)\n",
        "\n",
        "        output, hidden = rnn(current_input, [1], hidden=hidden)\n",
        "    \n",
        "    return sampled_string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlU73odMlbrR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "outputId": "f5fd3fae-9cad-42f5-97be-35de008afd47"
      },
      "source": [
        "epochs_number = 8\n",
        "\n",
        "\n",
        "for epoch_number in range(epochs_number):\n",
        "\n",
        "    for batch in trainset_loader:\n",
        "\n",
        "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "\n",
        "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
        "\n",
        "        output_sequences_batch_var =  output_sequences_batch.contiguous().view(-1).cuda()\n",
        "        input_sequences_batch_var = input_sequences_batch.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        logits, _ = rnn(input_sequences_batch_var, sequences_lengths)\n",
        "        \n",
        "        \n",
        "        train_loss = criterion(logits, output_sequences_batch_var)\n",
        "        \n",
        "        train_loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "    \n",
        "    \n",
        "    val_loss_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      \n",
        "      for batch in valset_loader:\n",
        "\n",
        "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "\n",
        "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
        "\n",
        "        output_sequences_batch_var =  output_sequences_batch.contiguous().view(-1).cuda()\n",
        "        input_sequences_batch_var = input_sequences_batch.cuda()\n",
        "        \n",
        "        logits, _ = rnn(input_sequences_batch_var, sequences_lengths)\n",
        "        loss = criterion(logits, output_sequences_batch_var)\n",
        "        \n",
        "        val_loss_list.append(loss.item())\n",
        "      \n",
        "    liveloss.update({'Validation loss': sum(val_loss_list) / len(val_loss_list),\n",
        "                     'Training loss': train_loss.item()})\n",
        "    liveloss.draw()\n",
        "    \n",
        "    print('Example of a text generated by current model:')\n",
        "    print(sample_from_rnn(starting_string='I', temperature=0.5))\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss:\n",
            "training   (min:    1.211, max:    1.740, cur:    1.211)\n",
            "\n",
            "Training loss:\n",
            "training   (min:    1.162, max:    1.795, cur:    1.162)\n",
            "Example of a text generated by current model:\n",
            "I can release the shadows  \n",
            "The way I could hold you  \n",
            "You can see it  \n",
            "  \n",
            "And I'm gonna stay me  \n",
            "The way you were my heart  \n",
            "  \n",
            "And I don't go  \n",
            "In the deside  \n",
            "Now I'm going down  \n",
            "You got the way that anywhere  \n",
            "  \n",
            "The end of my eyes  \n",
            "There's so wrong  \n",
            "  \n",
            "The day I can't hear you  \n",
            "I don't know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmeuHzsHf4pS"
      },
      "source": [
        "<center> <h2> Text Generation </h2> </center>\n",
        "\n",
        "During the text generation process we can define a starting prefix of characters that our model will condition on and generate the rest of the sequence. \n",
        "\n",
        "The model generates each new token (characters in our case) by sampling from the output probability distribution. When sampling, we can set a `temperature` parameter that controls the randomness of the sampling process. When this parameter approaches zero, the sampling is equivalent to argmax and when it is close to infinity the sampling is equivalent to sampling from a uniform distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASBCO64tnBkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7814e0ad-b0f8-4425-d3f1-35b83449d78d"
      },
      "source": [
        "def sample_from_rnn(starting_string=\"Why\", sample_length=300, temperature=1):\n",
        "    assert temperature >= 0.0\n",
        "    sampled_string = starting_string\n",
        "    hidden = None\n",
        "\n",
        "    first_input = torch.LongTensor( string_to_labels(starting_string) ).cuda()\n",
        "    first_input = first_input.unsqueeze(1)\n",
        "    current_input = first_input\n",
        "\n",
        "    output, hidden = rnn(current_input, [len(sampled_string)], hidden=hidden)\n",
        "\n",
        "    output = output[-1, :].unsqueeze(0)\n",
        "\n",
        "    for i in range(sample_length):\n",
        "\n",
        "        output_dist = nn.functional.softmax( output.view(-1).div(temperature + 1e-4) ).data\n",
        "\n",
        "        predicted_label = torch.multinomial(output_dist, 1)\n",
        "\n",
        "        sampled_string += all_characters[int(predicted_label[0])]\n",
        "\n",
        "        current_input = predicted_label.unsqueeze(1)\n",
        "\n",
        "        output, hidden = rnn(current_input, [1], hidden=hidden)\n",
        "    \n",
        "    return sampled_string\n",
        "\n",
        "#TODO? Try sampling with different temperatures and starting sequences\n",
        "# What do you observe? (See the pdf)\n",
        "print(sample_from_rnn(starting_string='I', temperature=0.5))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I think I say  \n",
            "  \n",
            "Can't see it  \n",
            "And I said it all be  \n",
            "  \n",
            "I can take the floor  \n",
            "And I want to say  \n",
            "I don't have to feel like I can do  \n",
            "Can you want to see  \n",
            "There are the hearts the best  \n",
            "All the season  \n",
            "I got the boy we have another  \n",
            "The blood of my heart  \n",
            "I want to do  \n",
            "  \n",
            "I want to stay  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3M1ge3miqXv"
      },
      "source": [
        "<center> <h2> Conditional Model (Optional) </h2> </center>\n",
        "So far our RNN has been trained on lyrics from all the songs without knowledge of which artists.\n",
        "\n",
        "Next, we will give the RNN additional information, the class representing the artist who made the song.\n",
        "\n",
        "Below we provide updated functions and dataset class that can be used to train language model that is conditioned on the artist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moBszIsDinEW"
      },
      "source": [
        "class LyricsGenerationDatasetConditional(data.Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file_path, minimum_song_count=None, artists=None):\n",
        "        \n",
        "        \n",
        "        self.lyrics_dataframe = pd.read_csv(csv_file_path)\n",
        "        \n",
        "        if artists:\n",
        "            \n",
        "            self.lyrics_dataframe = self.lyrics_dataframe[self.lyrics_dataframe.artist.isin(artists)]\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        if minimum_song_count:\n",
        "        \n",
        "            # Getting artists that have 70+ songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.groupby('artist').filter(lambda x: len(x) > minimum_song_count)\n",
        "            # Reindex .loc after we fetched random songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "        \n",
        "        # Get the length of the biggest lyric text\n",
        "        # We will need that for padding\n",
        "        self.max_text_len = self.lyrics_dataframe.text.str.len().max()\n",
        "        \n",
        "        whole_dataset_len = len(self.lyrics_dataframe)\n",
        "        \n",
        "        self.indexes = range(whole_dataset_len)\n",
        "        \n",
        "        \n",
        "        # Let's get unique artists and form a list\n",
        "        self.artists_list = list(self.lyrics_dataframe.artist.unique())\n",
        "        \n",
        "        # We will need the overall number of artists for \n",
        "        self.number_of_artists = len(self.artists_list)\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.indexes)\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        index = self.indexes[index]\n",
        "        \n",
        "        sequence_raw_string = self.lyrics_dataframe.loc[index].text\n",
        "        \n",
        "        sequence_string_labels = string_to_labels(sequence_raw_string)\n",
        "        \n",
        "        sequence_length = len(sequence_string_labels) - 1\n",
        "        \n",
        "        # Shifted by one char\n",
        "        input_string_labels = sequence_string_labels[:-1]\n",
        "        output_string_labels = sequence_string_labels[1:]\n",
        "                \n",
        "        # pad sequence so that all of them have the same lenght\n",
        "        # Otherwise the batching won't work\n",
        "        input_string_labels_padded = pad_sequence(input_string_labels, max_length=self.max_text_len)\n",
        "        \n",
        "        output_string_labels_padded = pad_sequence(output_string_labels, max_length=self.max_text_len, pad_label=-100)\n",
        "        \n",
        "        ## Adding the artist label\n",
        "        sequence_raw_artist_name_string = self.lyrics_dataframe.loc[index].artist\n",
        "\n",
        "        sequence_artist_label = self.artists_list.index(sequence_raw_artist_name_string)\n",
        "        \n",
        "        return (torch.LongTensor(input_string_labels_padded),\n",
        "                torch.LongTensor(output_string_labels_padded),\n",
        "                torch.LongTensor([sequence_artist_label]),\n",
        "                torch.LongTensor([sequence_length]) )\n",
        "\n",
        "    \n",
        "def post_process_sequence_batch_conditional(batch_tuple):\n",
        "    \n",
        "    input_sequences, output_sequences, artists, lengths = batch_tuple\n",
        "    \n",
        "    splitted_input_sequence_batch = input_sequences.split(split_size=1)\n",
        "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
        "    splitted_artists_batch = artists.split(split_size=1)\n",
        "    splitted_lengths_batch = lengths.split(split_size=1)\n",
        "\n",
        "    training_data_tuples = zip(splitted_input_sequence_batch,\n",
        "                               splitted_output_sequence_batch,\n",
        "                               splitted_artists_batch,\n",
        "                               splitted_lengths_batch)\n",
        "\n",
        "    training_data_tuples_sorted = sorted(training_data_tuples,\n",
        "                                         key=lambda p: int(p[3]),\n",
        "                                         reverse=True)\n",
        "\n",
        "    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_artists_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
        "\n",
        "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
        "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
        "    artists_batch_sorted = torch.cat(splitted_artists_batch)\n",
        "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
        "    \n",
        "    \n",
        "    # Here we trim overall data matrix using the size of the longest sequence\n",
        "    input_sequence_batch_sorted = input_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "    output_sequence_batch_sorted = output_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0]]\n",
        "    \n",
        "    # We should probably repeat this over the whole input sequence\n",
        "    artists_batch_sorted = artists_batch_sorted.expand_as(input_sequence_batch_sorted)\n",
        "\n",
        "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
        "    artists_batch_sorted_transposed = artists_batch_sorted.transpose(0, 1)\n",
        "    \n",
        "    # pytorch's api for rnns wants lenghts to be list of ints\n",
        "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
        "    lengths_batch_sorted_list = list(map(lambda x: int(x), lengths_batch_sorted_list))\n",
        "    \n",
        "    return input_sequence_batch_transposed, output_sequence_batch_sorted, artists_batch_sorted_transposed, lengths_batch_sorted_list\n",
        "\n",
        "\n",
        "class RNN_Conditional(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, num_classes, num_conditions, n_layers=2):\n",
        "        \n",
        "        super(RNN_Conditional, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.num_conditions = num_conditions\n",
        "        \n",
        "        # Converts labels into one-hot encoding and runs a linear\n",
        "        # layer on each of the converted one-hot encoded elements\n",
        "        \n",
        "        # input_size -- size of the dictionary + 1 (accounts for padding constant)\n",
        "        \n",
        "        \n",
        "        self.characters_embedder = # TODO: complete this line\n",
        "        \n",
        "        self.artist_embedder = # TODO: complete this line\n",
        "        \n",
        "        self.lstm = nn.LSTM(hidden_size * 2, #, #) # TODO: complete this line\n",
        "        \n",
        "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    \n",
        "    def forward(self, input_sequences, input_sequences_conditions, input_sequences_lengths, hidden=None):\n",
        "        \n",
        "        batch_size = input_sequences.shape[1]\n",
        "\n",
        "        characters_embedded = self.characters_embedder(input_sequences)\n",
        "        conditions_embedded = self.artist_embedder(input_sequences_conditions)\n",
        "        \n",
        "        embedded_combined = torch.cat((characters_embedded, conditions_embedded), dim=2)\n",
        "\n",
        "        # Here we run rnns only on non-padded regions of the batch\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded_combined, input_sequences_lengths)\n",
        "        outputs, hidden = self.lstm(packed, hidden)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
        "        \n",
        "        logits = self.logits_fc(outputs)\n",
        "        \n",
        "        logits = logits.transpose(0, 1).contiguous()\n",
        "        \n",
        "        logits_flatten = logits.view(-1, self.num_classes)\n",
        "        \n",
        "        return logits_flatten, hidden\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2M_XXJOnkkH"
      },
      "source": [
        "def sample_from_rnn_conditionally(starting_string=\"Why\", sample_length=300, temperature=1, artist_label=0):\n",
        "    \n",
        "    sampled_string = starting_string\n",
        "    hidden = None\n",
        "\n",
        "    first_input = torch.LongTensor( string_to_labels(starting_string) ).cuda()\n",
        "    first_input = first_input.unsqueeze(1)\n",
        "\n",
        "    # Expand the artist label to have the same size as input sequence\n",
        "    # we duplicate it in every input\n",
        "    artist_label_input = torch.LongTensor([artist_label]).expand_as(first_input)\n",
        "\n",
        "    current_sequence_input = first_input\n",
        "    current_artist_input = artist_label_input.cuda()\n",
        "\n",
        "    output, hidden = rnn(current_sequence_input, current_artist_input, [len(sampled_string)], hidden=hidden)\n",
        "\n",
        "    output = output[-1, :].unsqueeze(0)\n",
        "\n",
        "    for i in range(sample_length):\n",
        "\n",
        "        output_dist = nn.functional.softmax( output.view(-1).div(temperature) ).data\n",
        "\n",
        "        predicted_label = torch.multinomial(output_dist, 1)\n",
        "\n",
        "        sampled_string += all_characters[int(predicted_label[0])]\n",
        "        current_sequence_input = predicted_label.unsqueeze(1)\n",
        "\n",
        "        artist_label_input = torch.LongTensor([artist_label]).expand_as(current_sequence_input)\n",
        "        current_artist_input = artist_label_input.cuda()\n",
        "\n",
        "        output, hidden = rnn(current_sequence_input, current_artist_input, [1], hidden=hidden)\n",
        "    \n",
        "    return sampled_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcPuMC_PlR24"
      },
      "source": [
        "trainset = LyricsGenerationDatasetConditional(csv_file_path='songdata.csv', artists=artists)\n",
        "\n",
        "trainset_loader = torch.utils.data.DataLoader(trainset,\n",
        "                                              batch_size=50,\n",
        "                                              shuffle=True, num_workers=4, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Td2XXulwkz"
      },
      "source": [
        "rnn = RNN_Conditional(input_size=len(all_characters) + 1,\n",
        "          hidden_size=512,\n",
        "          num_classes=len(all_characters),\n",
        "          num_conditions=trainset.number_of_artists)\n",
        "\n",
        "rnn.cuda()\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eayQjtpZmTwU"
      },
      "source": [
        "from livelossplot import PlotLosses\n",
        "\n",
        "liveloss_train = PlotLosses()\n",
        "liveloss_val = PlotLosses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5yU-5anl4st",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "b6350b59-e36a-439d-e6b7-320d93b82aa8"
      },
      "source": [
        "# Conditional model takes more to converge,\n",
        "# Try training for more epochs for better results\n",
        "epochs_number = 5000\n",
        "\n",
        "for epoch_number in range(epochs_number):\n",
        "\n",
        "    for batch in trainset_loader:\n",
        "\n",
        "        post_processed_batch_tuple = post_process_sequence_batch_conditional(batch)\n",
        "\n",
        "        input_sequences_batch, output_sequences_batch, artists_batch, sequences_lengths = post_processed_batch_tuple\n",
        "\n",
        "        output_sequences_batch_var = output_sequences_batch.contiguous().view(-1).cuda()\n",
        "        \n",
        "        \n",
        "        input_sequences_batch_var = input_sequences_batch.cuda()\n",
        "        artists_batch_var = artists_batch.cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits, _ = rnn(input_sequences_batch_var, artists_batch_var, sequences_lengths)\n",
        "        \n",
        "        loss = criterion(logits, output_sequences_batch_var)\n",
        "        loss.backward()\n",
        "        \n",
        "        liveloss_train.update({'Training loss': loss.item()})\n",
        "        liveloss_train.draw()\n",
        "\n",
        "        #torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "    #print(sample_from_rnn_conditionally(artist_label=trainset.artists_list.index(\"Kanye West\")))\n",
        "    torch.save(rnn.state_dict(), 'conditional_rnn.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAE1CAYAAAB+0062AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss:\n",
            "training   (min:    1.273, max:    4.598, cur:    1.332)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e442d17510a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sequences_batch_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mliveloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqX7LVeNveSm"
      },
      "source": [
        "print(sample_from_rnn_conditionally(artist_label=trainset.artists_list.index(\"Eminem\"), starting_sting='A', temperature=0.5))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}